{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZ1Pip+zKpBwInhaYw3pUr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qhansen628/Predictive-Vision-Model-Experiments/blob/main/recurrent_attention_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Description\n",
        "\n",
        "-input:\n",
        "- Image patch location\n",
        "- Image patch flattened\n",
        "\n",
        "output:\n",
        "- classification label prediction (y_hat)\n",
        "- next location choice (x_loc)\n",
        "- next location patch prediction (image_patch prediction)\n",
        "- hidden activations"
      ],
      "metadata": {
        "id": "nGBJKajDdlEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fuck it we do step at a time\n",
        "\n",
        "Reconstruction network:\n",
        "- given predefined sequence of patches\n",
        "- RNN output reconstructed entire image x at each timestep\n",
        "\n",
        "Idea:\n",
        "-"
      ],
      "metadata": {
        "id": "SZY2ovpXO3xJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def extract_and_shuffle_patches(images, patch_size, stride):\n",
        "    batch_size, height, width, channels = images.shape\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=images,\n",
        "        sizes=[1, patch_size, patch_size, 1],\n",
        "        strides=[1, stride, stride, 1],\n",
        "        rates=[1, 1, 1, 1],\n",
        "        padding='SAME'\n",
        "    )\n",
        "    # Calculate number of patches and reshape to (batch_size, num_patches, patch_size*patch_size*channels)\n",
        "    num_patches_per_row = (height + stride - 1) // stride\n",
        "    num_patches_per_col = (width + stride - 1) // stride\n",
        "    num_patches = num_patches_per_row * num_patches_per_col\n",
        "    patches = tf.reshape(patches, [batch_size, num_patches, patch_size * patch_size * channels])\n",
        "\n",
        "    # Ensure patches are float32 if needed (commonly images are in float32)\n",
        "    patches = tf.cast(patches, tf.float32)\n",
        "\n",
        "    # Prepare positions and duplicate\n",
        "    positions = tf.eye(num_patches, dtype=tf.float32)  # One-hot encoding of positions\n",
        "\n",
        "    # Duplicate patches and positions\n",
        "    patches_doubled = tf.concat([patches, patches], axis=1)\n",
        "    positions_doubled = tf.concat([positions, positions], axis=0)\n",
        "\n",
        "    # Shuffle patches and positions within each batch\n",
        "    shuffled_indices = tf.map_fn(lambda x: tf.random.shuffle(tf.range(2 * num_patches)), tf.range(batch_size), dtype=tf.int32)\n",
        "    shuffled_patches = tf.gather(patches_doubled, shuffled_indices, batch_dims=1)\n",
        "    shuffled_positions = tf.gather(positions_doubled, shuffled_indices, axis=0)\n",
        "\n",
        "    # Concatenate patches with positions\n",
        "    full_inputs = tf.concat([shuffled_patches, shuffled_positions], axis=2)\n",
        "    #full_inputs = tf.concat([patches_doubled, positions_doubled], axis=2)\n",
        "\n",
        "    return full_inputs, shuffled_positions, shuffled_indices\n",
        "\n",
        "# Example usage with MNIST\n",
        "(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train / 255.0  # Normalize\n",
        "x_train = x_train[..., tf.newaxis]  # Add channel dimension\n",
        "\n",
        "# Extract patches\n",
        "patch_size = 4\n",
        "stride = 4\n",
        "inputs, positions, indices = extract_and_shuffle_patches(x_train[:10], patch_size, stride)\n",
        "\n",
        "print(f\"Input shape: {inputs.shape}\")  # Should show a reduced sequence length\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEpR2oFUx_gd",
        "outputId": "4e5c22dc-14d1-4ad3-bd9d-10544522bbb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (10, 98, 65)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageReconstructionRNN(tf.keras.Model):\n",
        "    def __init__(self, patch_size, stride, input_dim, units, output_size):\n",
        "        super().__init__()\n",
        "        self.rnn = tf.keras.layers.SimpleRNN(units, return_sequences=False)\n",
        "        self.dense0 = tf.keras.layers.Dense(output_size//2, activation='relu')\n",
        "        self.dense = tf.keras.layers.Dense(output_size, activation='relu')  # Assuming normalized images\n",
        "\n",
        "    def call(self, inputs):\n",
        "        rnn_output = self.rnn(inputs)\n",
        "        l2 = self.dense0(rnn_output)\n",
        "        reconstructed_image = self.dense(l2)\n",
        "        return reconstructed_image\n",
        "\n",
        "# Initialize the model\n",
        "image_height, image_width = 28, 28  # For example, with MNIST\n",
        "patch_size = 4\n",
        "stride = 4\n",
        "units = 128\n",
        "output_size = image_height * image_width  # Flattened image reconstruction\n",
        "\n",
        "model = ImageReconstructionRNN(patch_size, stride, patch_size**2, units, output_size)\n"
      ],
      "metadata": {
        "id": "buLg92pxwNNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idea:\n",
        "\n",
        "have an rnn model recieve a sequence of patches/patch locations.\n",
        "\n",
        "Reconstruct entire image from context vector. not each patch separately...\n",
        "\n",
        "this reconstructed"
      ],
      "metadata": {
        "id": "lMKGeWRryMCt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "NLWTAh5Ede2N",
        "outputId": "d639c34a-5dd6-4ed4-fcac-e5c467ce37d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Training loss (for one batch): 0.0798\n",
            "1/1 [==============================] - 0s 28ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have a model defined as 'model' which takes input of shape [None, patch_size**2 + num_patches]\n",
        "# and outputs the reconstructed image flattened.\n",
        "\n",
        "# Assuming 'extract_and_shuffle_patches' function is defined correctly as discussed before\n",
        "# Initialize the model\n",
        "\n",
        "# image_height, image_width = 28, 28  # For example, with MNIST\n",
        "# patch_size = 4\n",
        "# stride = 4\n",
        "# units = 64\n",
        "# output_size = image_height * image_width  # Flattened image reconstruction\n",
        "\n",
        "# model = ImageReconstructionRNN(patch_size, stride, patch_size**2, units, output_size)\n",
        "# Prepare dataset\n",
        "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_train = np.expand_dims(x_train, axis=-1)  # Add channel dimension\n",
        "\n",
        "# Extract and shuffle patches for the first batch as an example\n",
        "patch_size = 4\n",
        "stride = 4\n",
        "num_patches = (28 // stride) * (28 // stride)\n",
        "train_patches = extract_and_shuffle_patches(x_train[:32], patch_size, stride)\n",
        "\n",
        "# Prepare the target images (flattened)\n",
        "train_targets = x_train[:32].reshape(32, -1)  # Flatten the images\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    if epoch%10 == 0:  print(f'Epoch {epoch+1}/{epochs}')\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass through the model\n",
        "        predictions = model(train_patches)\n",
        "        # Compute the loss value\n",
        "        loss_value = loss_fn(train_targets, predictions)\n",
        "\n",
        "    # Use the gradient tape to automatically retrieve the gradients of the trainable variables with respect to the loss\n",
        "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "\n",
        "    # Run one step of gradient descent by updating the value of the variables to minimize the loss\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Log every epoch\n",
        "    if epoch%10 == 0: print(f\"Training loss (for one batch): {loss_value.numpy():.4f}\")\n",
        "\n",
        "# After training, you can visualize some reconstructed images\n",
        "predicted_images = model.predict(train_patches)\n",
        "num_images_to_show = 5\n",
        "\n",
        "for i in range(num_images_to_show):\n",
        "    # Original Image\n",
        "    plt.subplot(2, num_images_to_show, i + 1)\n",
        "    plt.imshow(x_train[i].squeeze(), cmap='gray')\n",
        "    plt.title(\"Original\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Reconstructed Image\n",
        "    plt.subplot(2, num_images_to_show, num_images_to_show + i + 1)\n",
        "    plt.imshow(predicted_images[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(\"Reconstructed\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set mask to 0"
      ],
      "metadata": {
        "id": "bxGNVJKlnb7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def extract_and_shuffle_patches(images, patch_size, stride, mask=0):\n",
        "    batch_size, height, width, channels = images.shape\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=images,\n",
        "        sizes=[1, patch_size, patch_size, 1],\n",
        "        strides=[1, stride, stride, 1],\n",
        "        rates=[1, 1, 1, 1],\n",
        "        padding='SAME'\n",
        "    )\n",
        "    # Calculate number of patches and reshape to (batch_size, num_patches, patch_features)\n",
        "    num_patches = (height // stride) * (width // stride)\n",
        "    patch_features = patch_size * patch_size * channels\n",
        "    patches = tf.reshape(patches, [batch_size, num_patches, patch_features])\n",
        "\n",
        "    # Create a one-hot encoded positions matrix\n",
        "    positions = tf.eye(num_patches, dtype=patches.dtype)  # One-hot encoding of positions\n",
        "    positions = tf.tile(positions[tf.newaxis, :, :], [batch_size, 1, 1])  # Expand to batch size\n",
        "\n",
        "    # Concatenate patches with positions\n",
        "    full_inputs = tf.concat([patches, positions], axis=-1)\n",
        "\n",
        "    # Shuffle patches and positions within each batch\n",
        "    shuffled_inputs = tf.map_fn(tf.random.shuffle, full_inputs)\n",
        "\n",
        "    # Determine how many patches to keep based on the mask percentage\n",
        "    num_patches_to_keep = num_patches - int(num_patches * mask / 100)\n",
        "\n",
        "    # Select a subset of patch-position pairs for each image in the batch\n",
        "    reduced_inputs = shuffled_inputs[:, :num_patches_to_keep, :]\n",
        "\n",
        "    return reduced_inputs\n",
        "\n",
        "# Example usage with MNIST\n",
        "(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_train = np.expand_dims(x_train, axis=-1)  # Add channel dimension\n",
        "\n",
        "# Extract patches with a 50% mask\n",
        "patch_size = 4\n",
        "stride = 4\n",
        "mask = 60  # Mask 50% of the patches\n",
        "inputs = extract_and_shuffle_patches(x_train[:10], patch_size, stride, mask)\n",
        "\n",
        "print(f\"Input shape: {inputs.shape}\")  # Should show a reduced sequence length\n"
      ],
      "metadata": {
        "id": "oTkX3h_BZxBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "589cba61-e721-45e2-c989-8ab9c53c9565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (10, 20, 65)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'extract_and_shuffle_patches' and 'ImageReconstructionRNN' are defined as discussed\n",
        "# Prepare dataset\n",
        "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train.astype('float32') / 255.0, x_test.astype('float32') / 255.0\n",
        "x_train, x_test = np.expand_dims(x_train, axis=-1), np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "# Extract and shuffle patches for training\n",
        "train_patches= extract_and_shuffle_patches(x_train, patch_size, stride)\n",
        "train_targets = x_train.reshape(-1, image_height * image_width)\n",
        "\n",
        "# Initialize the model\n",
        "model = ImageReconstructionRNN(patch_size, stride, patch_size**2 + num_patches, units, output_size)\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Training loop\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    # Training\n",
        "    model.fit(train_patches, train_targets, epochs=1, verbose=0)\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch + 1}/{epochs}')\n",
        "\n",
        "# Visualization after training\n",
        "num_images_to_show = 5\n",
        "test_indices = np.random.choice(x_test.shape[0], num_images_to_show, replace=False)\n",
        "test_patches, _, _ = extract_and_shuffle_patches(x_test[test_indices], patch_size, stride)\n",
        "\n",
        "predicted_images = model.predict(test_patches)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(num_images_to_show):\n",
        "    # Original Image\n",
        "    plt.subplot(2, num_images_to_show, i + 1)\n",
        "    plt.imshow(x_test[test_indices[i]].squeeze(), cmap='gray')\n",
        "    plt.title(\"Original\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Reconstructed Image\n",
        "    plt.subplot(2, num_images_to_show, num_images_to_show + i + 1)\n",
        "    plt.imshow(predicted_images[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(\"Reconstructed\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HexgZXybpnu",
        "outputId": "eb3df9d1-d80d-431a-9b05-1c559ad59cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Epoch 11/100\n",
            "Epoch 21/100\n",
            "Epoch 31/100\n",
            "Epoch 41/100\n",
            "Epoch 51/100\n",
            "Epoch 61/100\n",
            "Epoch 71/100\n",
            "Epoch 81/100\n",
            "Epoch 91/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-6bffac657176>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mnum_images_to_show\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mtest_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_images_to_show\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtest_patches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_and_shuffle_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mpredicted_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8m1Wer2j8VEt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}